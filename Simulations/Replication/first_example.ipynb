{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:00:02.758328Z",
     "start_time": "2025-02-28T05:00:01.682066Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from CART import *\n",
    "from Utils.plotting import  *\n",
    "from scipy.stats import norm as ndist\n",
    "import joblib\n",
    "\n",
    "# For tree-values\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "\n",
    "# Select a CRAN mirror to download from\n",
    "utils = rpackages.importr('utils')\n",
    "utils.chooseCRANmirror(ind=1)  # Select the first mirror\n",
    "\n",
    "# Install 'remotes' if it's not already installed\n",
    "if not rpackages.isinstalled('remotes'):\n",
    "    utils.install_packages(StrVector(('remotes',)))\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects import numpy2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Skipping install of 'treevalues' from a github remote, the SHA1 (55573782) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the GitHub installation command for 'treevalues'\n",
    "ro.r('remotes::install_github(\"anna-neufeld/treevalues\")')\n",
    "ro.r('library(treevalues)')\n",
    "ro.r('library(rpart)')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:00:03.315872Z",
     "start_time": "2025-02-28T05:00:03.072271Z"
    }
   },
   "id": "b5d5ac8ed92d6131"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def generate_test(mu, sd_y):\n",
    "    n = mu.shape[0]\n",
    "    return mu + np.random.normal(size=(n,), scale=sd_y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:00:03.330426Z",
     "start_time": "2025-02-28T05:00:03.317418Z"
    }
   },
   "id": "2b12e4284552b1ef"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "class RegressionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=float('inf'),\n",
    "                 min_proportion=0.2, min_bucket=5):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        self.min_proportion = min_proportion\n",
    "        self.min_bucket = min_bucket\n",
    "        self.terminal_nodes = []\n",
    "        self.terminal_parents = []\n",
    "    def fit(self, X, y, sd=1):\n",
    "        # sd is std. dev. of randomization\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n = X.shape[0]\n",
    "        self.root = self._build_tree(X, y, sd=sd)\n",
    "        #print(\"Fit sd:\", sd)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0, membership=None,\n",
    "                    prev_branch=None, sd=1.):\n",
    "        #print(\"Build tree sd:\", sd)\n",
    "        \"\"\"\n",
    "        A recursive private function to build the tree\n",
    "        by repeatedly splitting\n",
    "        :param X: the covariates of the previous sub-region\n",
    "        :param y: the response of the previous sub-region\n",
    "        :param depth: depth of the previous split\n",
    "        :param sd: std. dev. of randomization\n",
    "        :return: a node characterizing this split and fitted value\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        if depth == 0:\n",
    "            membership = np.ones((num_samples,))\n",
    "        else:\n",
    "            assert membership is not None\n",
    "\n",
    "        if prev_branch is None:\n",
    "            prev_branch = []\n",
    "            # print(\"pbc:\", prev_branch)\n",
    "\n",
    "        if num_samples >= max(self.min_samples_split, 2) and depth < self.max_depth:\n",
    "            best_split = self._get_best_split(X, y, num_features, sd_rand=sd)\n",
    "            if \"feature_index\" not in best_split.keys():\n",
    "                print(best_split)\n",
    "                print(X)\n",
    "            feature_idx = best_split[\"feature_index\"]\n",
    "            threshold = best_split[\"threshold\"]\n",
    "            pos = best_split[\"position\"]\n",
    "            left_mbsp = self.X[:, feature_idx] <= threshold\n",
    "            right_mbsp = self.X[:, feature_idx] > threshold\n",
    "            left_mbsp = left_mbsp * membership  # n x 1 logical vector\n",
    "            right_mbsp = right_mbsp * membership  # n x 1 logical vector\n",
    "            # if best_split[\"gain\"] > 0:\n",
    "            left_prev_branch = prev_branch.copy()\n",
    "            left_prev_branch.append([feature_idx, pos, 0])\n",
    "            right_prev_branch = prev_branch.copy()\n",
    "            right_prev_branch.append([feature_idx, pos, 1])\n",
    "            # print(left_prev_branch)\n",
    "            # print(right_prev_branch)\n",
    "            left_subtree \\\n",
    "                = self._build_tree(best_split[\"X_left\"],\n",
    "                                   best_split[\"y_left\"],\n",
    "                                   depth + 1,\n",
    "                                   membership=left_mbsp,\n",
    "                                   prev_branch=left_prev_branch,\n",
    "                                   sd=sd)\n",
    "            right_subtree \\\n",
    "                = self._build_tree(best_split[\"X_right\"],\n",
    "                                   best_split[\"y_right\"],\n",
    "                                   depth + 1,\n",
    "                                   membership=right_mbsp,\n",
    "                                   prev_branch=right_prev_branch,\n",
    "                                   sd=sd)\n",
    "\n",
    "            leaf_value = self._calculate_leaf_value(y)\n",
    "            cur_node = TreeNode(value=leaf_value,\n",
    "                                feature_index=best_split[\"feature_index\"],\n",
    "                                threshold=best_split[\"threshold\"],\n",
    "                                pos=pos,\n",
    "                                left=left_subtree, right=right_subtree,\n",
    "                                membership=membership, depth=depth,\n",
    "                                randomization=best_split[\"randomization\"],\n",
    "                                prev_branch=prev_branch,\n",
    "                                sd_rand=sd, terminal=False)\n",
    "            # Add this parent node to subnodes\n",
    "            left_subtree.prev_node = cur_node\n",
    "            right_subtree.prev_node = cur_node\n",
    "            if left_subtree.terminal and right_subtree.terminal:\n",
    "                #print(cur_node.threshold)\n",
    "                self.terminal_parents.append(cur_node)\n",
    "            return cur_node\n",
    "        leaf_value = self._calculate_leaf_value(y)\n",
    "        cur_node = TreeNode(value=leaf_value, membership=membership,\n",
    "                            sd_rand=sd, depth=depth, terminal=True)\n",
    "        self.terminal_nodes.append(cur_node)\n",
    "        return cur_node\n",
    "\n",
    "    def _get_best_split(self, X, y, num_features, sd_rand=1):\n",
    "        \"\"\"\n",
    "        Input (X, y) of a (potentially sub-)region, return information about\n",
    "        the best split on this regions\n",
    "        Assuming no ties in features\n",
    "        :param X: the (sub-)region's covariates\n",
    "        :param y: the (sub-)region's response\n",
    "        :param num_features: dimension of X\n",
    "        :return: a dictionary containing\n",
    "                {split_feature_idx, (numerical) splitting_threshold,\n",
    "                split_position, left_sub_region, right_sub_region,\n",
    "                gain}\n",
    "        \"\"\"\n",
    "        best_split = {}\n",
    "        min_loss = float('inf')\n",
    "        num_sample = X.shape[0]\n",
    "        randomization = np.zeros((num_sample - 1, num_features))\n",
    "        min_proportion = self.min_proportion\n",
    "        # Override min_proportion if min_bucket is set\n",
    "        if self.min_bucket is not None:\n",
    "            start = self.min_bucket\n",
    "            end = num_sample - self.min_bucket - 1\n",
    "        else:\n",
    "            start = int(np.floor(num_sample * min_proportion))\n",
    "            end = num_sample - int(np.ceil(num_sample * min_proportion)) - 1\n",
    "        #print(start, end)\n",
    "        #print(\"Get best split sd:\", sd_rand)\n",
    "\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = X[:, feature_index]\n",
    "            feature_values_sorted = feature_values.copy()\n",
    "            feature_values_sorted.sort()\n",
    "            #for i in range(len(feature_values_sorted) - 1):\n",
    "            for i in range(start, end):\n",
    "                threshold = feature_values_sorted[i]\n",
    "                X_left, y_left, X_right, y_right = self._split(X, y, feature_index, threshold)\n",
    "                if len(X_left) > 0 and len(X_right) > 0:\n",
    "                    #print(\"entered 1\")\n",
    "                    if sd_rand != 0:\n",
    "                        omega = np.random.normal(scale=sd_rand)\n",
    "                    else:\n",
    "                        omega = 0\n",
    "                    randomization[i, feature_index] = omega\n",
    "                    loss = self._calculate_loss(y_left, y_right, omega)\n",
    "                    if loss < min_loss:\n",
    "                        #print(\"entered 2\")\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"position\"] = i\n",
    "                        best_split[\"X_left\"] = X_left\n",
    "                        best_split[\"y_left\"] = y_left\n",
    "                        best_split[\"X_right\"] = X_right\n",
    "                        best_split[\"y_right\"] = y_right\n",
    "                        best_split[\"loss\"] = loss\n",
    "                        best_split[\"randomization\"] = randomization\n",
    "                        # best_split[\"\"]\n",
    "                        min_loss = loss\n",
    "        return best_split\n",
    "\n",
    "    def _split(self, X, y, feature_index, threshold):\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = X[:, feature_index] > threshold\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "    def _calculate_information_gain(self, y, y_left, y_right):\n",
    "        var_total = np.var(y) * len(y)\n",
    "        var_left = np.var(y_left) * len(y_left)\n",
    "        var_right = np.var(y_right) * len(y_right)\n",
    "        return var_total - (var_left + var_right)\n",
    "\n",
    "    def _calculate_loss(self, y_left, y_right, randomization):\n",
    "        n1 = len(y_left)\n",
    "        n2 = len(y_right)\n",
    "        n = n1 + n2\n",
    "        \"\"\"loss = ((np.var(y_left) * n1 + np.var(y_right) * np.sqrt(n2)) / np.sqrt(n1 + n2)\n",
    "                + randomization)\"\"\"\n",
    "        loss = ( (- n1 * np.mean(y_left) ** 2 - n2 * np.mean(y_right) ** 2) / np.sqrt(n)\n",
    "                + randomization)\n",
    "        # Actually need not divide by n1+n2...\n",
    "        #print(\"loss:\", loss - randomization)\n",
    "        #print(\"randomization:\", randomization)\n",
    "        return loss\n",
    "\n",
    "    #\n",
    "    def _calculate_leaf_value(self, y):\n",
    "        \"\"\"\n",
    "        :param y: the response of the previous sub-region\n",
    "        :return: the mean of the region\n",
    "        \"\"\"\n",
    "        return np.mean(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: the test dataset\n",
    "        :return: fitted values\n",
    "        \"\"\"\n",
    "        return np.array([self._predict(sample, self.root) for sample in X])\n",
    "\n",
    "    def _predict(self, sample, tree):\n",
    "        \"\"\"\n",
    "        Recursively searching the tree for the surrounding region of `sample`\n",
    "        :param sample: the input covariates\n",
    "        :param tree: the trained tree\n",
    "        :return: fitted y value of `sample`\n",
    "        \"\"\"\n",
    "        if tree.terminal:\n",
    "            return tree.value\n",
    "        feature_value = sample[tree.feature_index]\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self._predict(sample, tree.left)\n",
    "        else:\n",
    "            return self._predict(sample, tree.right)\n",
    "\n",
    "    def _approx_log_reference(self, node, grid, nuisance,\n",
    "                              contrast, norm_contrast, sd=1, sd_rand=1):\n",
    "        ## TODO: 0. grid is a grid for eta'Y / (sd * norm_contrast);\n",
    "        ##          first reconstruct eta'Y and then reconstruct Q\n",
    "        ## TODO: 1. reconstruct Q from the grid\n",
    "        ## TODO: 2. Perform Laplace approximation for each grid,\n",
    "        #           and for each node split\n",
    "        ## TODO: 3. Add back the constant term omitted in Laplace Approximation\n",
    "        ## TODO: 4. Return reference measure\n",
    "\n",
    "        prev_branch = node.prev_branch.copy()\n",
    "        current_depth = node.depth\n",
    "        ref_hat = np.zeros_like(grid)\n",
    "\n",
    "        ## TODO: Move the node according to branch when evaluating integrals\n",
    "        node = self.root\n",
    "\n",
    "        # norm = np.linalg.norm(contrast)\n",
    "        depth = 0\n",
    "\n",
    "        while depth <= current_depth:\n",
    "            for g_idx, g in enumerate(grid):\n",
    "                y_grid = g * sd ** 2 * norm_contrast + nuisance\n",
    "                # TODO: Account for depth here\n",
    "                # Subsetting the covariates to this current node\n",
    "                X = self.X[node.membership.astype(bool)]\n",
    "                y_g = y_grid[node.membership.astype(bool)]\n",
    "                y_node = self.y[node.membership.astype(bool)]\n",
    "                y_left = y_grid[node.left.membership.astype(bool)]\n",
    "                y_right = y_grid[node.right.membership.astype(bool)]\n",
    "                y_left_obs = self.y[node.left.membership.astype(bool)]\n",
    "                y_right_obs = self.y[node.right.membership.astype(bool)]\n",
    "                optimal_loss = self._calculate_loss(y_left, y_right,\n",
    "                                                    randomization=0)\n",
    "                opt_loss_obs = self._calculate_loss(y_left_obs, y_right_obs,\n",
    "                                                    randomization=0)\n",
    "                j_opt = node.feature_index  # j^*\n",
    "                s_opt = node.pos  # s^*\n",
    "                randomization = node.randomization\n",
    "                S_total, J_total = randomization.shape\n",
    "                implied_mean = []\n",
    "                observed_opt = []\n",
    "\n",
    "\n",
    "\n",
    "                # TODO: Add a layer to account for depth of the tree\n",
    "                for j in range(J_total):\n",
    "                    feature_values = X[:, j]\n",
    "                    feature_values_sorted = feature_values.copy()\n",
    "                    feature_values_sorted.sort()\n",
    "                    for s in range(S_total - 1):\n",
    "                        if not (j == j_opt and s == s_opt):\n",
    "                            threshold = feature_values_sorted[s]\n",
    "                            X_left, y_left, X_right, y_right \\\n",
    "                                = self._split(X, y_g, j, threshold)\n",
    "                            implied_mean_s_j \\\n",
    "                                = optimal_loss - self._calculate_loss(y_left,\n",
    "                                                                      y_right,\n",
    "                                                                      randomization=0)\n",
    "                            # The split of the actually observed Y\n",
    "                            X_left_o, y_left_o, X_right_o, y_right_o \\\n",
    "                                = self._split(X, y_node, j, threshold)\n",
    "                            # print(y_left_o.shape)\n",
    "                            # print(y_right_o.shape)\n",
    "                            observed_opt_s_j = (opt_loss_obs -\n",
    "                                                self._calculate_loss(y_left_o,\n",
    "                                                                     y_right_o,\n",
    "                                                                     randomization=0)\n",
    "                                                + (randomization[s_opt, j_opt] -\n",
    "                                                   randomization[s, j]))\n",
    "                            # print(\"s:\", s, \"j:\", j, \"sopt:\", s_opt, \"jopt:\", j_opt)\n",
    "\n",
    "                            # Record the implied mean\n",
    "                            # and observed optimization variable\n",
    "                            implied_mean.append(implied_mean_s_j)\n",
    "                            observed_opt.append(observed_opt_s_j)\n",
    "\n",
    "                # The implied mean is given by the optimal loss minus\n",
    "                # the loss at each split\n",
    "                implied_mean = np.array(implied_mean)\n",
    "                observed_opt = np.array(observed_opt)\n",
    "                # print(observed_opt)\n",
    "                assert np.max(observed_opt) < 0\n",
    "\n",
    "                # dimension of the optimization variable\n",
    "                n_opt = len(implied_mean)\n",
    "                implied_cov = np.ones((n_opt, n_opt)) + np.eye(n_opt)\n",
    "                prec = (np.eye(n_opt) - np.ones((n_opt, n_opt))\n",
    "                        / ((n_opt + 1))) / (sd_rand ** 2)\n",
    "\n",
    "                # TODO: what is a feasible point?\n",
    "                # TODO: Need to have access to the observed opt var\n",
    "                #       where we actually pass in g = eta'Y.\n",
    "                # print(\"Implied mean\", implied_mean)\n",
    "                # print(\"feasible point\", observed_opt)\n",
    "                # print(\"prec\", prec)\n",
    "                # Approximate the selection probability\n",
    "                sel_prob, _, _ = solve_barrier_tree_nonneg(Q=implied_mean,\n",
    "                                                           precision=prec,\n",
    "                                                           feasible_point=None)\n",
    "                const_term = (implied_mean).T.dot(prec).dot(implied_mean) / 2\n",
    "                ref_hat[g_idx] += (- sel_prob - const_term)\n",
    "                print(\"conjugate norm:\", np.linalg.norm(prec.dot(implied_mean)))\n",
    "\n",
    "            # Move to the next layer\n",
    "            if depth < current_depth:\n",
    "                dir = prev_branch[depth][2]\n",
    "                if dir == 0:\n",
    "                    node = node.left  # Depend on where the branch demands\n",
    "                else:\n",
    "                    node = node.right\n",
    "                depth += 1\n",
    "            else:\n",
    "                depth += 1  # Exit the loop if targeting depth achieved\n",
    "\n",
    "        return np.array(ref_hat)\n",
    "\n",
    "    def _condl_approx_log_reference(self, node, grid, nuisance,\n",
    "                                    norm_contrast, sd=1, sd_rand=1,\n",
    "                                    reduced_dim=5, use_CVXPY=True):\n",
    "        ## TODO: 0. grid is a grid for eta'Y / (sd * ||contrast||_2);\n",
    "        ##          first reconstruct eta'Y and then reconstruct Q\n",
    "        ## TODO: 1. reconstruct Q from the grid\n",
    "        ## TODO: 2. Perform Laplace approximation for each grid,\n",
    "        #           and for each node split\n",
    "        ## TODO: 3. Add back the constant term omitted in Laplace Approximation\n",
    "        ## TODO: 4. Return reference measure\n",
    "        \n",
    "        r_is_none = reduced_dim is None\n",
    "\n",
    "        def k_dim_prec(k, sd_rand):\n",
    "            prec = (np.eye(k) - np.ones((k, k))\n",
    "                    / ((k + 1))) / (sd_rand ** 2)\n",
    "            #print(\"Precision (k-dim):\", prec)\n",
    "            #print(\"SD_rand:\", sd_rand)\n",
    "            return prec\n",
    "\n",
    "        def get_cond_dist(mean, cov, cond_idx, rem_idx, rem_val,\n",
    "                          sd_rand, rem_dim):\n",
    "            prec_rem = k_dim_prec(k=rem_dim, sd_rand=sd_rand)\n",
    "\n",
    "            cond_mean = mean[cond_idx] + cov[np.ix_(cond_idx, rem_idx)].dot(prec_rem).dot(rem_val - mean[rem_idx])\n",
    "            cond_cov = cov[np.ix_(cond_idx, cond_idx)] - cov[np.ix_(cond_idx, rem_idx)].dot(prec_rem).dot(\n",
    "                cov[np.ix_(rem_idx, cond_idx)])\n",
    "            cond_prec = np.linalg.inv(cond_cov)\n",
    "\n",
    "            return cond_mean, cond_cov, cond_prec\n",
    "\n",
    "        def get_log_pdf(observed_opt, implied_mean, rem_idx, sd_rand, rem_dim):\n",
    "            x = observed_opt[rem_idx]\n",
    "            mean = implied_mean[rem_idx]\n",
    "\n",
    "            return -1/2 * (np.linalg.norm(x-mean)**2 - np.sum(x-mean)**2/(rem_dim+1)) / sd_rand**2\n",
    "\n",
    "        prev_branch = node.prev_branch.copy()\n",
    "        current_depth = node.depth\n",
    "        ref_hat = np.zeros_like(grid)\n",
    "\n",
    "        node = self.root\n",
    "\n",
    "        # norm = np.linalg.norm(contrast)\n",
    "        depth = 0\n",
    "\n",
    "        while depth <= current_depth:\n",
    "            # Subsetting the covariates to this current node\n",
    "            X = self.X[node.membership.astype(bool)]\n",
    "            j_opt = node.feature_index  # j^*\n",
    "            s_opt = node.pos  # s^*\n",
    "            randomization = node.randomization\n",
    "            S_total, J_total = randomization.shape\n",
    "\n",
    "            # Sort feature values to get the threshold\n",
    "            feature_values_sorted = np.zeros_like(X)\n",
    "            for j in range(J_total):\n",
    "                feature_values_sorted[:, j] = X[:, j].copy()\n",
    "                feature_values_sorted[:, j].sort()\n",
    "\n",
    "            for g_idx, g in enumerate(grid):\n",
    "                # norm_contrast: eta / (||eta|| * sigma)\n",
    "                # grid is a grid for eta'y / (||eta|| * sigma)\n",
    "                y_grid = g * sd ** 2 * norm_contrast + nuisance\n",
    "\n",
    "                # Reconstructing y\n",
    "                y_g = y_grid[node.membership.astype(bool)]\n",
    "                y_node = self.y[node.membership.astype(bool)]\n",
    "                y_left = y_grid[node.left.membership.astype(bool)]\n",
    "                y_right = y_grid[node.right.membership.astype(bool)]\n",
    "                y_left_obs = self.y[node.left.membership.astype(bool)]\n",
    "                y_right_obs = self.y[node.right.membership.astype(bool)]\n",
    "                optimal_loss = self._calculate_loss(y_left, y_right,\n",
    "                                                    randomization=0)\n",
    "                opt_loss_obs = self._calculate_loss(y_left_obs, y_right_obs,\n",
    "                                                    randomization=0)\n",
    "\n",
    "                implied_mean = []\n",
    "                observed_opt = []\n",
    "\n",
    "                # Iterate over all covariates\n",
    "                for j in range(J_total):\n",
    "                    num_sample = X.shape[0]\n",
    "                    min_proportion = self.min_proportion\n",
    "                    # Override min_proportion if min_bucket is set\n",
    "                    if self.min_bucket is not None:\n",
    "                        start = self.min_bucket\n",
    "                        end = num_sample - self.min_bucket - 1\n",
    "                    else:\n",
    "                        start = int(np.floor(num_sample * min_proportion))\n",
    "                        end = num_sample - int(np.ceil(num_sample * min_proportion)) - 1\n",
    "\n",
    "                    # for s in range(S_total - 1):\n",
    "                    for s in range(start, end):\n",
    "                        if not (j == j_opt and s == s_opt):\n",
    "                            threshold = feature_values_sorted[s,j]\n",
    "                            X_left, y_left, X_right, y_right \\\n",
    "                                = self._split(X, y_g, j, threshold)\n",
    "                            implied_mean_s_j \\\n",
    "                                = optimal_loss - self._calculate_loss(y_left,\n",
    "                                                                      y_right,\n",
    "                                                                      randomization=0)\n",
    "                            # The split of the actually observed Y\n",
    "                            X_left_o, y_left_o, X_right_o, y_right_o \\\n",
    "                                = self._split(X, y_node, j, threshold)\n",
    "                            # print(y_left_o.shape)\n",
    "                            # print(y_right_o.shape)\n",
    "                            observed_opt_s_j = (opt_loss_obs -\n",
    "                                                self._calculate_loss(y_left_o,\n",
    "                                                                     y_right_o,\n",
    "                                                                     randomization=0)\n",
    "                                                + (randomization[s_opt, j_opt] -\n",
    "                                                   randomization[s, j]))\n",
    "                            # print(\"s:\", s, \"j:\", j, \"sopt:\", s_opt, \"jopt:\", j_opt)\n",
    "\n",
    "                            # Record the implied mean\n",
    "                            # and observed optimization variable\n",
    "                            implied_mean.append(implied_mean_s_j)\n",
    "                            observed_opt.append(observed_opt_s_j)\n",
    "\n",
    "                # The implied mean is given by the optimal loss minus\n",
    "                # the loss at each split\n",
    "                implied_mean = np.array(implied_mean)\n",
    "                observed_opt = np.array(observed_opt)\n",
    "                if np.max(observed_opt) >= 0:\n",
    "                    print(observed_opt)\n",
    "                assert np.max(observed_opt) < 0\n",
    "                \n",
    "                if r_is_none:\n",
    "                    reduced_dim = int(len(implied_mean) * 0.3)\n",
    "                    #print(reduced_dim)\n",
    "                \n",
    "                # Get the order of optimization variables in descending order\n",
    "                obs_opt_order = np.argsort(observed_opt)[::-1]\n",
    "                #print(\"obs_opt_order\", len(obs_opt_order))\n",
    "                #print(\"unique vals\", len(np.unique(obs_opt_order)))\n",
    "                # reduced_dim = max(int(0.1*len(implied_mean)), 5)\n",
    "                top_d_idx = obs_opt_order[0:reduced_dim]\n",
    "                rem_d_idx = obs_opt_order[reduced_dim:]\n",
    "                offset_val = observed_opt[obs_opt_order[reduced_dim]]\n",
    "                # print(\"LB:\", offset_val)\n",
    "\n",
    "                linear = np.zeros((reduced_dim * 2, reduced_dim))\n",
    "                linear[0:reduced_dim, 0:reduced_dim] = np.eye(reduced_dim)\n",
    "                linear[reduced_dim:, 0:reduced_dim] = -np.eye(reduced_dim)\n",
    "                offset = np.zeros(reduced_dim * 2)\n",
    "                offset[reduced_dim:] = -offset_val\n",
    "                # dimension of the optimization variable\n",
    "                n_opt = len(implied_mean)\n",
    "                implied_cov = (np.ones((n_opt, n_opt)) + np.eye(n_opt)) * (sd_rand ** 2)\n",
    "                cond_implied_mean, cond_implied_cov, cond_implied_prec = (\n",
    "                    get_cond_dist(mean=implied_mean,\n",
    "                                  cov=implied_cov,\n",
    "                                  cond_idx=top_d_idx,\n",
    "                                  rem_idx=rem_d_idx,\n",
    "                                  rem_val=observed_opt[rem_d_idx],\n",
    "                                  sd_rand=sd_rand,\n",
    "                                  rem_dim=n_opt - reduced_dim))\n",
    "\n",
    "                if use_CVXPY:\n",
    "                    ### USE CVXPY\n",
    "                    # Define the variable\n",
    "                    o = cp.Variable(reduced_dim)\n",
    "                    # print(n_opt)\n",
    "                    # print(len(cond_implied_mean))\n",
    "\n",
    "                    # Objective function: (1/2) * (u - Q)' * A * (u - Q)\n",
    "                    objective = cp.Minimize(0.5 * cp.quad_form(o - cond_implied_mean,\n",
    "                                                               cond_implied_prec))\n",
    "                    # Constraints: con_linear' * u <= con_offset\n",
    "                    constraints = [o >= offset_val, o <= 0]\n",
    "                    # print(offset_val)\n",
    "                    # Problem definition\n",
    "                    prob = cp.Problem(objective, constraints)\n",
    "                    # Solve the problem\n",
    "                    prob.solve()\n",
    "                    ref_hat[g_idx] += (-prob.value)\n",
    "                    # Add omitted term\n",
    "                    ref_hat[g_idx] += (get_log_pdf(observed_opt=observed_opt,\n",
    "                                                   implied_mean=implied_mean,\n",
    "                                                   rem_idx=rem_d_idx,\n",
    "                                                   sd_rand=sd_rand,\n",
    "                                                   rem_dim=n_opt - reduced_dim))\n",
    "                else:\n",
    "                    sel_prob, _, _ = solve_barrier_tree_box_PGD(Q=cond_implied_mean,\n",
    "                                                                precision=cond_implied_prec,\n",
    "                                                                lb=offset_val,\n",
    "                                                                feasible_point=None)\n",
    "                    const_term = (cond_implied_mean).T.dot(cond_implied_prec).dot(cond_implied_mean) / 2\n",
    "                    ref_hat[g_idx] += (- sel_prob - const_term)\n",
    "                    # Add omitted term\n",
    "                    ref_hat[g_idx] += (get_log_pdf(observed_opt=observed_opt,\n",
    "                                                   implied_mean=implied_mean,\n",
    "                                                   rem_idx=rem_d_idx,\n",
    "                                                   sd_rand=sd_rand,\n",
    "                                                   rem_dim=n_opt - reduced_dim))\n",
    "\n",
    "            # Move to the next layer\n",
    "            if depth < current_depth:\n",
    "                dir = prev_branch[depth][2]\n",
    "                if dir == 0:\n",
    "                    node = node.left  # Depend on where the branch demands\n",
    "                else:\n",
    "                    node = node.right\n",
    "                depth += 1\n",
    "            else:\n",
    "                depth += 1  # Exit the loop if targeting depth achieved\n",
    "\n",
    "        return np.array(ref_hat)\n",
    "\n",
    "    def split_inference(self, node, ngrid=1000, ncoarse=20, grid_width=15,\n",
    "                        sd=1, level=0.9):\n",
    "        \"\"\"\n",
    "        Inference for a split of a node\n",
    "        :param node: the node whose split is of interest\n",
    "        :return: p-values for difference in mean\n",
    "        \"\"\"\n",
    "        # First determine the projection direction\n",
    "        left_membership = node.left.membership\n",
    "        right_membership = node.right.membership\n",
    "        contrast = left_membership / np.sum(left_membership) - right_membership / np.sum(right_membership)\n",
    "        sd_rand = node.sd_rand\n",
    "\n",
    "        norm_contrast = contrast / (np.linalg.norm(contrast) * sd)\n",
    "\n",
    "        # Using the normalized contrast in practice\n",
    "        # for scale-free grid approximation\n",
    "        observed_target = norm_contrast @ self.y\n",
    "        # The nuisance parameter is defined the same way\n",
    "        # as on papers\n",
    "        nuisance = (self.y - np.linalg.outer(contrast, contrast)\n",
    "                    @ self.y / (np.linalg.norm(contrast) ** 2))\n",
    "\n",
    "        stat_grid = np.linspace(-grid_width, grid_width,\n",
    "                                num=ngrid)\n",
    "\n",
    "        if ncoarse is not None:\n",
    "            coarse_grid = np.linspace(-grid_width, grid_width, ncoarse)\n",
    "            eval_grid = coarse_grid\n",
    "        else:\n",
    "            eval_grid = stat_grid\n",
    "\n",
    "        # Evaluate reference measure (selection prob.) over stat_grid\n",
    "        ref = self._approx_log_reference(node=node,\n",
    "                                         grid=eval_grid,\n",
    "                                         nuisance=nuisance,\n",
    "                                         contrast=contrast,\n",
    "                                         norm_contrast=norm_contrast, sd=1,\n",
    "                                         sd_rand=sd_rand)\n",
    "\n",
    "        if ncoarse is None:\n",
    "            logWeights = np.zeros((ngrid,))\n",
    "            for g in range(ngrid):\n",
    "                # Evaluate the log pdf as a sum of (log) gaussian pdf\n",
    "                # and (log) reference measure\n",
    "                # TODO: Check if the original exp. fam. density is correct\n",
    "                logWeights[g] = (- 0.5 * (stat_grid[g]) ** 2 + ref[g])\n",
    "            # normalize logWeights\n",
    "            logWeights = logWeights - np.max(logWeights)\n",
    "            condl_density = discrete_family(eval_grid,\n",
    "                                            np.exp(logWeights),\n",
    "                                            logweights=logWeights)\n",
    "        else:\n",
    "            # print(\"Coarse grid\")\n",
    "            approx_fn = interp1d(eval_grid,\n",
    "                                 ref,\n",
    "                                 kind='quadratic',\n",
    "                                 bounds_error=False,\n",
    "                                 fill_value='extrapolate')\n",
    "            grid = np.linspace(-grid_width, grid_width, num=ngrid)\n",
    "            sel_probs = np.zeros((ngrid,))\n",
    "            logWeights = np.zeros((ngrid,))\n",
    "            for g in range(ngrid):\n",
    "                # TODO: Check if the original exp. fam. density is correct\n",
    "                logWeights[g] = (- 0.5 * (grid[g]) ** 2 + approx_fn(grid[g]))\n",
    "                sel_probs[g] = approx_fn(grid[g])\n",
    "\n",
    "            # normalize logWeights\n",
    "            logWeights = logWeights - np.max(logWeights)\n",
    "\n",
    "            condl_density = discrete_family(grid, np.exp(logWeights),\n",
    "                                            logweights=logWeights)\n",
    "\n",
    "        if np.isnan(logWeights).sum() != 0:\n",
    "            print(\"logWeights contains nan\")\n",
    "        elif (logWeights == np.inf).sum() != 0:\n",
    "            print(\"logWeights contains inf\")\n",
    "        elif (np.asarray(ref) == np.inf).sum() != 0:\n",
    "            print(\"ref contains inf\")\n",
    "        elif (np.asarray(ref) == -np.inf).sum() != 0:\n",
    "            print(\"ref contains -inf\")\n",
    "        elif np.isnan(np.asarray(ref)).sum() != 0:\n",
    "            print(\"ref contains nan\")\n",
    "\n",
    "        \"\"\"interval = (condl_density.equal_tailed_interval\n",
    "                        (observed=contrast.T @ self.y,\n",
    "                         alpha=1-level))\n",
    "        if np.isnan(interval[0]) or np.isnan(interval[1]):\n",
    "            print(\"Failed to construct intervals: nan\")\"\"\"\n",
    "\n",
    "        # TODO: Fix this; pass in observed values\n",
    "        pivot = condl_density.ccdf(x=observed_target\n",
    "                                     / (np.linalg.norm(contrast) * sd),\n",
    "                                   theta=0)\n",
    "\n",
    "        return (pivot, condl_density, contrast, norm_contrast,\n",
    "                observed_target, logWeights, sel_probs)\n",
    "\n",
    "    def condl_split_inference(self, node, ngrid=1000, ncoarse=20, grid_w_const=1.5,\n",
    "                              sd=1, reduced_dim=5, use_cvxpy=False):\n",
    "        \"\"\"\n",
    "        Inference for a split of a node\n",
    "        :param node: the node whose split is of interest\n",
    "        :return: p-values for difference in mean\n",
    "        \"\"\"\n",
    "        # First determine the projection direction\n",
    "        left_membership = node.left.membership\n",
    "        right_membership = node.right.membership\n",
    "        contrast = left_membership / np.sum(left_membership) - right_membership / np.sum(right_membership)\n",
    "        sd_rand = node.sd_rand\n",
    "\n",
    "        # Normalized contrast: The inner product norm_contrast'Y has sd = 1.\n",
    "        norm_contrast = contrast / (np.linalg.norm(contrast) * sd)\n",
    "\n",
    "        # Using the normalized contrast in practice\n",
    "        # for scale-free grid approximation\n",
    "        observed_target = norm_contrast @ self.y\n",
    "        # The nuisance parameter is defined the same way\n",
    "        # as on papers\n",
    "        nuisance = (self.y - np.linalg.outer(contrast, contrast)\n",
    "                    @ self.y / (np.linalg.norm(contrast) ** 2))\n",
    "\n",
    "        grid_width = grid_w_const * np.abs(observed_target)\n",
    "\n",
    "        stat_grid = np.linspace(-grid_width, grid_width, num=ngrid)\n",
    "\n",
    "        if ncoarse is not None:\n",
    "            coarse_grid = np.linspace(-grid_width, grid_width, ncoarse)\n",
    "            eval_grid = coarse_grid\n",
    "        else:\n",
    "            eval_grid = stat_grid\n",
    "\n",
    "        ref = self._condl_approx_log_reference(node=node,\n",
    "                                               grid=eval_grid,\n",
    "                                               nuisance=nuisance,\n",
    "                                               norm_contrast=norm_contrast, sd=sd,\n",
    "                                               sd_rand=sd_rand,\n",
    "                                               reduced_dim=reduced_dim,\n",
    "                                               use_CVXPY=use_cvxpy)\n",
    "\n",
    "        if ncoarse is None:\n",
    "            logWeights = np.zeros((ngrid,))\n",
    "            for g in range(ngrid):\n",
    "                # Evaluate the log pdf as a sum of (log) gaussian pdf\n",
    "                # and (log) reference measure\n",
    "                # TODO: Check if the original exp. fam. density is correct\n",
    "                logWeights[g] = (- 0.5 * (stat_grid[g]) ** 2 + ref[g])\n",
    "            # normalize logWeights\n",
    "            logWeights = logWeights - np.max(logWeights)\n",
    "            condl_density = discrete_family(eval_grid,\n",
    "                                            np.exp(logWeights),\n",
    "                                            logweights=logWeights)\n",
    "        else:\n",
    "            # print(\"Coarse grid\")\n",
    "            approx_fn = interp1d(eval_grid,\n",
    "                                 ref,\n",
    "                                 kind='quadratic',\n",
    "                                 bounds_error=False,\n",
    "                                 fill_value='extrapolate')\n",
    "            grid = np.linspace(-grid_width, grid_width, num=ngrid)\n",
    "            logWeights = np.zeros((ngrid,))\n",
    "            suff = np.zeros((ngrid,))\n",
    "            sel_probs = np.zeros((ngrid,))\n",
    "            for g in range(ngrid):\n",
    "                # TODO: Check if the original exp. fam. density is correct\n",
    "\n",
    "                logWeights[g] = (- 0.5 * (grid[g]) ** 2 + approx_fn(grid[g]))\n",
    "                suff[g] = - 0.5 * (grid[g]) ** 2\n",
    "                sel_probs[g] = approx_fn(grid[g])\n",
    "\n",
    "            # normalize logWeights\n",
    "            logWeights = logWeights - np.max(logWeights)\n",
    "\n",
    "            # condl_density is a discrete approximation\n",
    "            # to the exponential family distribution with\n",
    "            # natural parameter theta := eta'mu / (||eta|| * sigma)\n",
    "            # and\n",
    "            # sufficient statistic X := eta'y / (||eta|| * sigma) = norm_contrast'Y\n",
    "            condl_density = discrete_family(grid, np.exp(logWeights),\n",
    "                                            logweights=logWeights)\n",
    "\n",
    "        if np.isnan(logWeights).sum() != 0:\n",
    "            print(\"logWeights contains nan\")\n",
    "        elif (logWeights == np.inf).sum() != 0:\n",
    "            print(\"logWeights contains inf\")\n",
    "        elif (np.asarray(ref) == np.inf).sum() != 0:\n",
    "            print(\"ref contains inf\")\n",
    "        elif (np.asarray(ref) == -np.inf).sum() != 0:\n",
    "            print(\"ref contains -inf\")\n",
    "        elif np.isnan(np.asarray(ref)).sum() != 0:\n",
    "            print(\"ref contains nan\")\n",
    "\n",
    "        \"\"\"interval = (condl_density.equal_tailed_interval\n",
    "                        (observed=contrast.T @ self.y,\n",
    "                         alpha=1-level))\n",
    "        if np.isnan(interval[0]) or np.isnan(interval[1]):\n",
    "            print(\"Failed to construct intervals: nan\")\"\"\"\n",
    "\n",
    "        # TODO: Fix this; pass in observed values\n",
    "        pivot = condl_density.ccdf(x=observed_target,\n",
    "                                   theta=0)\n",
    "\n",
    "        \"\"\"# Recall: observed_target = norm_contrast @ self.y\n",
    "        L, U = condl_density.equal_tailed_interval(observed=observed_target,\n",
    "                                                   alpha=0.1)\n",
    "\n",
    "        print('CI:', L, ',', U)\"\"\"\n",
    "\n",
    "        return (pivot, condl_density, contrast, norm_contrast,\n",
    "                observed_target, logWeights, suff, sel_probs)\n",
    "\n",
    "    def condl_node_inference(self, node, ngrid=1000, ncoarse=20, grid_w_const=1.5,\n",
    "                             sd=1, reduced_dim=5, use_cvxpy=False):\n",
    "        \"\"\"\n",
    "        Inference for a split of a node\n",
    "        :param node: the node whose split is of interest\n",
    "        :return: p-values for difference in mean\n",
    "        \"\"\"\n",
    "        # First determine the projection direction\n",
    "        membership = node.membership\n",
    "        contrast = membership / np.sum(membership)\n",
    "        sd_rand = node.sd_rand\n",
    "        #print(\"Inference sd\", sd_rand)\n",
    "\n",
    "        # Normalized contrast: The inner product norm_contrast'Y has sd = 1.\n",
    "        norm_contrast = contrast / (np.linalg.norm(contrast) * sd)\n",
    "\n",
    "        # Using the normalized contrast in practice\n",
    "        # for scale-free grid approximation\n",
    "        observed_target = norm_contrast @ self.y\n",
    "        # The nuisance parameter is defined the same way\n",
    "        # as on papers\n",
    "        nuisance = (self.y - np.linalg.outer(contrast, contrast)\n",
    "                    @ self.y / (np.linalg.norm(contrast) ** 2))\n",
    "\n",
    "        grid_width = grid_w_const * np.abs(observed_target)\n",
    "\n",
    "        stat_grid = np.linspace(-grid_width, grid_width, num=ngrid)\n",
    "\n",
    "        if ncoarse is not None:\n",
    "            coarse_grid = np.linspace(-grid_width, grid_width, ncoarse)\n",
    "            eval_grid = coarse_grid\n",
    "        else:\n",
    "            eval_grid = stat_grid\n",
    "\n",
    "        ref = self._condl_approx_log_reference(node=node.prev_node,\n",
    "                                               grid=eval_grid,\n",
    "                                               nuisance=nuisance,\n",
    "                                               norm_contrast=norm_contrast, sd=sd,\n",
    "                                               sd_rand=sd_rand,\n",
    "                                               reduced_dim=reduced_dim,\n",
    "                                               use_CVXPY=use_cvxpy)\n",
    "\n",
    "        if ncoarse is None:\n",
    "            logWeights = np.zeros((ngrid,))\n",
    "            for g in range(ngrid):\n",
    "                # Evaluate the log pdf as a sum of (log) gaussian pdf\n",
    "                # and (log) reference measure\n",
    "                # TODO: Check if the original exp. fam. density is correct\n",
    "                logWeights[g] = (- 0.5 * (stat_grid[g]) ** 2 + ref[g])\n",
    "            # normalize logWeights\n",
    "            logWeights = logWeights - np.max(logWeights)\n",
    "            condl_density = discrete_family(eval_grid,\n",
    "                                            np.exp(logWeights),\n",
    "                                            logweights=logWeights)\n",
    "        else:\n",
    "            # print(\"Coarse grid\")\n",
    "            approx_fn = interp1d(eval_grid,\n",
    "                                 ref,\n",
    "                                 kind='quadratic',\n",
    "                                 bounds_error=False,\n",
    "                                 fill_value='extrapolate')\n",
    "            grid = np.linspace(-grid_width, grid_width, num=ngrid)\n",
    "            logWeights = np.zeros((ngrid,))\n",
    "            suff = np.zeros((ngrid,))\n",
    "            sel_probs = np.zeros((ngrid,))\n",
    "            for g in range(ngrid):\n",
    "                # TODO: Check if the original exp. fam. density is correct\n",
    "\n",
    "                logWeights[g] = (- 0.5 * (grid[g]) ** 2 + approx_fn(grid[g]))\n",
    "                suff[g] = - 0.5 * (grid[g]) ** 2\n",
    "                sel_probs[g] = approx_fn(grid[g])\n",
    "\n",
    "            # normalize logWeights\n",
    "            logWeights = logWeights - np.max(logWeights)\n",
    "\n",
    "            # condl_density is a discrete approximation\n",
    "            # to the exponential family distribution with\n",
    "            # natural parameter theta := eta'mu / (||eta|| * sigma)\n",
    "            # and\n",
    "            # sufficient statistic X := eta'y / (||eta|| * sigma) = norm_contrast'Y\n",
    "            condl_density = discrete_family(grid, np.exp(logWeights),\n",
    "                                            logweights=logWeights)\n",
    "\n",
    "        if np.isnan(logWeights).sum() != 0:\n",
    "            print(\"logWeights contains nan\")\n",
    "        elif (logWeights == np.inf).sum() != 0:\n",
    "            print(\"logWeights contains inf\")\n",
    "        elif (np.asarray(ref) == np.inf).sum() != 0:\n",
    "            print(\"ref contains inf\")\n",
    "        elif (np.asarray(ref) == -np.inf).sum() != 0:\n",
    "            print(\"ref contains -inf\")\n",
    "        elif np.isnan(np.asarray(ref)).sum() != 0:\n",
    "            print(\"ref contains nan\")\n",
    "\n",
    "        \"\"\"interval = (condl_density.equal_tailed_interval\n",
    "                        (observed=contrast.T @ self.y,\n",
    "                         alpha=1-level))\n",
    "        if np.isnan(interval[0]) or np.isnan(interval[1]):\n",
    "            print(\"Failed to construct intervals: nan\")\"\"\"\n",
    "\n",
    "        # TODO: Fix this; pass in observed values\n",
    "        pivot = condl_density.ccdf(x=observed_target,\n",
    "                                   theta=0)\n",
    "\n",
    "        \"\"\"# Recall: observed_target = norm_contrast @ self.y\n",
    "        L, U = condl_density.equal_tailed_interval(observed=observed_target,\n",
    "                                                   alpha=0.1)\n",
    "\n",
    "        print('CI:', L, ',', U)\"\"\"\n",
    "\n",
    "        return (pivot, condl_density, contrast, norm_contrast,\n",
    "                observed_target, logWeights, suff, sel_probs)\n",
    "\n",
    "    def _delete_children(self, node):\n",
    "        \"\"\"\n",
    "        :param node: The node whose children are to be deleted\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        node.left = None\n",
    "        node.right = None\n",
    "        # Keep track of the terminal nodes\n",
    "        node.terminal = True\n",
    "\n",
    "\n",
    "    def bottom_up_pruning(self, level=0.1, sd_y=1):\n",
    "        temp_term_parents = []\n",
    "        while self.terminal_parents:\n",
    "            parent = self.terminal_parents.pop()\n",
    "            pivot, dist, contrast, norm_contrast, obs_tar, logW, suff, sel_probs = (\n",
    "                self.condl_split_inference(node=parent,\n",
    "                                           ngrid=10000,\n",
    "                                           ncoarse=200,\n",
    "                                           grid_w_const=2.5,\n",
    "                                           reduced_dim=1,\n",
    "                                           sd=sd_y,\n",
    "                                           use_cvxpy=True))\n",
    "\n",
    "            # Prune if the split is insignificant\n",
    "            if min(pivot, 1-pivot) >= level/2:\n",
    "                self._delete_children(parent)\n",
    "                if parent.prev_branch:\n",
    "                    if parent.prev_branch[-1][2] == 0:\n",
    "                        neighbor = parent.prev_node.right\n",
    "                    else:\n",
    "                        neighbor = parent.prev_node.left\n",
    "                    # If this parent node's parent is now a terminal parent node\n",
    "                    # add it to the terminal parents list\n",
    "                    if neighbor.terminal:\n",
    "                        self.terminal_parents.append(parent.prev_node)\n",
    "            else:\n",
    "                # If the split is significant,\n",
    "                # preserve it in the temp list\n",
    "                temp_term_parents.append(parent)\n",
    "\n",
    "        self.terminal_parents = temp_term_parents\n",
    "\n",
    "\n",
    "    def print_branches(self, node=None, start=True, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively printing (with proper indentation denoting depth) the tree\n",
    "        :param node: the node to be printed\n",
    "        :param start: a logic flag for whether the node is the root\n",
    "        :param depth: depth of a node to be printed\n",
    "        \"\"\"\n",
    "        if start:\n",
    "            node = self.root\n",
    "        if node is None:\n",
    "            return\n",
    "        if node.left or node.right:\n",
    "            print(\"\\t\" * depth, \"j:\", node.feature_index)\n",
    "            print(\"\\t\" * depth, \"threshold:\", node.threshold)\n",
    "            if node.left and node.right:\n",
    "                print(\"\\t\" * depth, \"left:\")\n",
    "                self.print_branches(node.left, start=False, depth=depth + 1)\n",
    "                print(\"\\t\" * depth, \"right:\")\n",
    "                self.print_branches(node.right, start=False, depth=depth + 1)\n",
    "            elif node.right:\n",
    "                print(\"\\t\" * depth, \"left:\")\n",
    "                self.print_branches(node.right, start=False, depth=depth + 1)\n",
    "            else:\n",
    "                print(\"\\t\" * depth, \"right:\")\n",
    "                self.print_branches(node.left, start=False, depth=depth + 1)\n",
    "        return\n",
    "\n",
    "    # Example usage:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:39:32.725688Z",
     "start_time": "2025-02-28T05:39:32.712519Z"
    }
   },
   "id": "56de2d348b1c5e8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tree-values inference"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T04:45:00.398703Z",
     "start_time": "2025-02-28T04:45:00.393620Z"
    }
   },
   "id": "3201be659cb7259f"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def tree_values_inference(X, y, mu, sd_y, max_depth=5, level=0.1,\n",
    "                          X_test=None):\n",
    "    # Convert the NumPy matrix to an R matrix\n",
    "    X_r = numpy2ri.py2rpy(X)\n",
    "    y_r = numpy2ri.py2rpy(y)\n",
    "\n",
    "    # Assign the R matrix to a variable in the R environment (optional)\n",
    "    ro.globalenv['X_r'] = X_r\n",
    "    ro.globalenv['y_r'] = y_r\n",
    "    ro.globalenv['p'] = X.shape[1]\n",
    "\n",
    "    # Construct dataset\n",
    "    ro.r('data <- cbind(y_r, X_r)')\n",
    "    # Set the column names to \"y\", \"x1\", \"x2\", ..., \"x10\"\n",
    "    ro.r('colnames(data) <- c(\"y\", paste0(\"x\", 1:p))')\n",
    "    ro.r('data = data.frame(data)')\n",
    "\n",
    "    # Define the rpart tree model\n",
    "    tree_cmd = ('bls.tree <- rpart(y ~ ., data=data, model = TRUE, ' +\n",
    "                'control = rpart.control(cp=0.00, minsplit = 50, minbucket = 20, maxdepth=') + str(max_depth) + '))'\n",
    "    ro.r(tree_cmd)\n",
    "    bls_tree = ro.r('bls.tree')\n",
    "    # Plot the tree values (this will plot directly if you have a plotting backend set up)\n",
    "    # ro.r('treeval.plot(bls.tree, inferenceType=0)')\n",
    "\n",
    "    # ro.r('print(row.names(bls.tree$frame)[bls.tree$frame$var == \"<leaf>\"])')\n",
    "    ro.r('leaf_idx <- (row.names(bls.tree$frame)[bls.tree$frame$var == \"<leaf>\"])')\n",
    "    leaf_idx = ro.r['leaf_idx']\n",
    "\n",
    "    # Get node mapping\n",
    "    ro.r('idx_full <- 1:nrow(bls.tree$frame)')\n",
    "    ro.r('mapped_idx <- idx_full[bls.tree$frame$var == \"<leaf>\"]')\n",
    "\n",
    "    len = []\n",
    "    coverage = []\n",
    "    len_naive = []\n",
    "    coverage_naive = []\n",
    "\n",
    "    for i, idx in enumerate(leaf_idx):\n",
    "        # Get the branch information for a specific branch in the tree\n",
    "        command = 'branch <- getBranch(bls.tree, ' + str(idx) + ')'\n",
    "        ro.r(command)\n",
    "        # Perform branch inference\n",
    "        ro.r(f'result <- branchInference(bls.tree, branch, type=\"reg\", alpha = 0.10, sigma_y={sd_y})')\n",
    "        # Get confidence intervals\n",
    "        confint = ro.r('result$confint')\n",
    "        len.append(confint[1] - confint[0])\n",
    "\n",
    "        target_cmd = \"contrast <- (bls.tree$where == mapped_idx[\" + str(i + 1) + \"])\"\n",
    "        ro.r(target_cmd)\n",
    "        contrast = ro.r('contrast')\n",
    "        contrast = np.array(contrast)\n",
    "\n",
    "        contrast = np.array(contrast * 1 / np.sum(contrast))\n",
    "\n",
    "        target = contrast.dot(mu)\n",
    "        coverage.append(target >= confint[0] and target <= confint[1])\n",
    "\n",
    "        # Naive after tree value\n",
    "        # Confidence intervals\n",
    "        naive_CI = [contrast.dot(y) -\n",
    "                    np.linalg.norm(contrast) * sd_y * ndist.ppf(1 - level / 2),\n",
    "                    contrast.dot(y) +\n",
    "                    np.linalg.norm(contrast) * sd_y * ndist.ppf(1 - level / 2)]\n",
    "        coverage_naive.append((target >= naive_CI[0] and target <= naive_CI[1]))\n",
    "        len_naive.append(naive_CI[1] - naive_CI[0])\n",
    "\n",
    "    if X_test is not None:\n",
    "        X_test_r = numpy2ri.py2rpy(X_test)\n",
    "        ro.globalenv['X_test_r'] = X_test_r\n",
    "        ro.r('pred <- predict(bls.tree, data = X_test_r)')\n",
    "        pred = ro.r['pred']\n",
    "    else:\n",
    "        pred = None\n",
    "\n",
    "    return (np.mean(coverage), np.mean(len),\n",
    "            np.mean(coverage_naive), np.mean(len_naive), pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:39:34.092140Z",
     "start_time": "2025-02-28T05:39:34.085608Z"
    }
   },
   "id": "d5f59772261d8c06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RRT inference"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T04:45:00.826537Z",
     "start_time": "2025-02-28T04:45:00.817685Z"
    }
   },
   "id": "68b034d92f8e7a30"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def randomized_inference(reg_tree, sd_y, y, mu, level=0.1):\n",
    "    # print(reg_tree.terminal_nodes)\n",
    "    coverage_i = []\n",
    "    lengths_i = []\n",
    "\n",
    "    for node in reg_tree.terminal_nodes:\n",
    "        pval, dist, contrast, norm_contrast, obs_tar, logW, suff, sel_probs \\\n",
    "            = (reg_tree.condl_node_inference(node=node,\n",
    "                                             ngrid=10000,\n",
    "                                             ncoarse=50,\n",
    "                                             grid_w_const=5,\n",
    "                                             reduced_dim=None,\n",
    "                                             sd=sd_y,\n",
    "                                             use_cvxpy=True))\n",
    "        target = contrast.dot(mu)\n",
    "\n",
    "        # This is an interval for\n",
    "        # eta_*'mu = eta'mu / (norm(eta) * sd_y)\n",
    "        selective_CI = (dist.equal_tailed_interval(observed=norm_contrast.dot(y),\n",
    "                                                   alpha=level))\n",
    "        selective_CI = np.array(selective_CI)\n",
    "        selective_CI *= np.linalg.norm(contrast) * sd_y\n",
    "        coverage_i.append((target >= selective_CI[0] and target <= selective_CI[1]))\n",
    "        lengths_i.append(selective_CI[1] - selective_CI[0])\n",
    "\n",
    "    return coverage_i, lengths_i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:39:34.657183Z",
     "start_time": "2025-02-28T05:39:34.648219Z"
    }
   },
   "id": "593dfd6522f79486"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference with UV decomposition"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T04:45:01.348886Z",
     "start_time": "2025-02-28T04:45:01.342709Z"
    }
   },
   "id": "b59562660744af96"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def UV_decomposition(X, y, mu, sd_y,\n",
    "                     max_depth=5, min_prop=0, min_sample=10, min_bucket=5,\n",
    "                     level=0.1, gamma=1,\n",
    "                     X_test=None):\n",
    "    n = X.shape[0]\n",
    "    W = np.random.normal(loc=0, scale=sd_y * np.sqrt(gamma), size=(n,))\n",
    "    U = y + W\n",
    "    V = y - W / gamma\n",
    "    sd_V = sd_y * np.sqrt(1 + 1 / gamma)\n",
    "    reg_tree = RegressionTree(min_samples_split=min_sample, max_depth=max_depth,\n",
    "                              min_proportion=min_prop, min_bucket=min_bucket)\n",
    "    reg_tree.fit(X, U, sd=0)\n",
    "\n",
    "    coverage = []\n",
    "    lengths = []\n",
    "\n",
    "    for node in reg_tree.terminal_nodes:\n",
    "        contrast = node.membership\n",
    "\n",
    "        contrast = np.array(contrast * 1 / np.sum(contrast))\n",
    "\n",
    "        target = contrast.dot(mu)\n",
    "\n",
    "        # Naive after tree value\n",
    "        # Confidence intervals\n",
    "        CI = [contrast.dot(V) -\n",
    "              np.linalg.norm(contrast) * sd_V * ndist.ppf(1 - level / 2),\n",
    "              contrast.dot(V) +\n",
    "              np.linalg.norm(contrast) * sd_V * ndist.ppf(1 - level / 2)]\n",
    "        coverage.append((target >= CI[0] and target <= CI[1]))\n",
    "        lengths.append(CI[1] - CI[0])\n",
    "\n",
    "    if X_test is not None:\n",
    "        pred = reg_tree.predict(X_test)\n",
    "    else:\n",
    "        pred = None\n",
    "\n",
    "    return coverage, lengths, pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:39:35.339666Z",
     "start_time": "2025-02-28T05:39:35.335250Z"
    }
   },
   "id": "45c2e38de4e0ee17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Replicating Figure 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T04:45:02.220083Z",
     "start_time": "2025-02-28T04:45:02.195087Z"
    }
   },
   "id": "1e3f3b7319ac6822"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def terminal_inference_sim(n=50, p=5, a=0.1, b=0.1,\n",
    "                           sd_y=1,\n",
    "                           noise_sd_list=[0.5, 1, 2, 5],\n",
    "                           UV_gamma_list=[],\n",
    "                           use_nonrand=True,\n",
    "                           start=0, end=100,\n",
    "                           level=0.1, path=None):\n",
    "    method_list = [f\"RRT_{sd}\" for sd in noise_sd_list]\n",
    "    if use_nonrand:\n",
    "        method_list += [\"Tree val\", \"Naive\"]\n",
    "    for gamma in UV_gamma_list:\n",
    "        method_list.append(\"UV_\" + str(gamma))\n",
    "\n",
    "    coverage_dict = {m: [] for m in method_list}\n",
    "    length_dict = {m: [] for m in method_list}\n",
    "    MSE_dict = {m: [] for m in method_list}\n",
    "\n",
    "    for i in range(start, end):\n",
    "        print(i, \"th simulation\")\n",
    "        np.random.seed(i + 10000)\n",
    "        X = np.random.normal(size=(n, p))\n",
    "\n",
    "        mu = b * ((X[:, 0] <= 0) * (1 + a * (X[:, 1] > 0) + (X[:, 2] * X[:, 1] <= 0)))\n",
    "        y = mu + np.random.normal(size=(n,), scale=sd_y)\n",
    "        y_test = generate_test(mu, sd_y)\n",
    "        \n",
    "        if use_nonrand:\n",
    "            # Tree value & naive inference & prediction\n",
    "            (coverage_treeval, avg_len_treeval,\n",
    "             coverage_treeval_naive, avg_len_treeval_naive,\n",
    "             pred_test_treeval) = tree_values_inference(X, y, mu, sd_y=sd_y,\n",
    "                                                        X_test=X, max_depth=3)\n",
    "            MSE_test_treeval = (np.mean((y_test - pred_test_treeval) ** 2))\n",
    "\n",
    "            coverage_dict[\"Tree val\"].append(coverage_treeval)\n",
    "            length_dict[\"Tree val\"].append(avg_len_treeval)\n",
    "            MSE_dict[\"Tree val\"].append(MSE_test_treeval)\n",
    "            coverage_dict[\"Naive\"].append(coverage_treeval_naive)\n",
    "            length_dict[\"Naive\"].append(avg_len_treeval_naive)\n",
    "            MSE_dict[\"Naive\"].append(MSE_test_treeval)\n",
    "\n",
    "        for noise_sd in noise_sd_list:\n",
    "            # Create and train the regression tree\n",
    "            reg_tree = RegressionTree(min_samples_split=50, max_depth=3,\n",
    "                                      min_proportion=0., min_bucket=20)\n",
    "\n",
    "            reg_tree.fit(X, y, sd=noise_sd * sd_y)\n",
    "\n",
    "            coverage_i, lengths_i = randomized_inference(reg_tree=reg_tree,\n",
    "                                                         y=y, sd_y=sd_y, mu=mu,\n",
    "                                                         level=level)\n",
    "            pred_test = reg_tree.predict(X)\n",
    "            MSE_test = (np.mean((y_test - pred_test) ** 2))\n",
    "            # Record results\n",
    "            coverage_dict[f\"RRT_{noise_sd}\"].append(np.mean(coverage_i))\n",
    "            length_dict[f\"RRT_{noise_sd}\"].append(np.mean(lengths_i))\n",
    "            MSE_dict[f\"RRT_{noise_sd}\"].append(MSE_test)\n",
    "\n",
    "        for gamma in UV_gamma_list:\n",
    "            gamma_key = \"UV_\" + str(gamma)\n",
    "            # UV decomposition\n",
    "            coverage_UV, len_UV, pred_UV = UV_decomposition(X, y, mu, sd_y, X_test=X,\n",
    "                                                            min_prop=0., max_depth=3,\n",
    "                                                            min_sample=50, min_bucket=20,\n",
    "                                                            gamma=gamma)\n",
    "            MSE_UV = (np.mean((y_test - pred_UV) ** 2))\n",
    "            coverage_dict[gamma_key].append(np.mean(coverage_UV))\n",
    "            length_dict[gamma_key].append(np.mean(len_UV))\n",
    "            MSE_dict[gamma_key].append(MSE_UV)\n",
    "\n",
    "        if path is not None:\n",
    "            joblib.dump([coverage_dict, length_dict, MSE_dict], path, compress=1)\n",
    "\n",
    "    return coverage_dict, length_dict, MSE_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:39:35.853183Z",
     "start_time": "2025-02-28T05:39:35.846424Z"
    }
   },
   "id": "d4fc427721e488a9"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th simulation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "\n"
     ]
    },
    {
     "ename": "RRuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRRuntimeError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[78], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m coverage_dict_fig1, length_dict_fig1, MSE_dict_fig1\\\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;241m=\u001B[39m \u001B[43mterminal_inference_sim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                             \u001B[49m\u001B[43msd_y\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mnoise_sd_list\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mUV_gamma_list\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                             \u001B[49m\u001B[43muse_nonrand\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mstart\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[77], line 31\u001B[0m, in \u001B[0;36mterminal_inference_sim\u001B[0;34m(n, p, a, b, sd_y, noise_sd_list, UV_gamma_list, use_nonrand, start, end, level, path)\u001B[0m\n\u001B[1;32m     25\u001B[0m y_test \u001B[38;5;241m=\u001B[39m generate_test(mu, sd_y)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_nonrand:\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;66;03m# Tree value & naive inference & prediction\u001B[39;00m\n\u001B[1;32m     29\u001B[0m     (coverage_treeval, avg_len_treeval,\n\u001B[1;32m     30\u001B[0m      coverage_treeval_naive, avg_len_treeval_naive,\n\u001B[0;32m---> 31\u001B[0m      pred_test_treeval) \u001B[38;5;241m=\u001B[39m \u001B[43mtree_values_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msd_y\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msd_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mX_test\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_depth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     MSE_test_treeval \u001B[38;5;241m=\u001B[39m (np\u001B[38;5;241m.\u001B[39mmean((y_test \u001B[38;5;241m-\u001B[39m pred_test_treeval) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m))\n\u001B[1;32m     35\u001B[0m     coverage_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTree val\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(coverage_treeval)\n",
      "Cell \u001B[0;32mIn[74], line 44\u001B[0m, in \u001B[0;36mtree_values_inference\u001B[0;34m(X, y, mu, sd_y, max_depth, level, X_test)\u001B[0m\n\u001B[1;32m     42\u001B[0m ro\u001B[38;5;241m.\u001B[39mr(command)\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Perform branch inference\u001B[39;00m\n\u001B[0;32m---> 44\u001B[0m \u001B[43mro\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mr\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mresult <- branchInference(bls.tree, branch, type=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m, alpha = 0.10, sigma_y=\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43msd_y\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Get confidence intervals\u001B[39;00m\n\u001B[1;32m     46\u001B[0m confint \u001B[38;5;241m=\u001B[39m ro\u001B[38;5;241m.\u001B[39mr(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresult$confint\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/PhD/SI_Codes/SI-CART/lib/python3.9/site-packages/rpy2/robjects/__init__.py:509\u001B[0m, in \u001B[0;36mR.__call__\u001B[0;34m(self, string, invisible, print_r_warnings)\u001B[0m\n\u001B[1;32m    507\u001B[0m     invisible \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_invisible\n\u001B[1;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m invisible:\n\u001B[0;32m--> 509\u001B[0m     res, visible \u001B[38;5;241m=\u001B[39m \u001B[43mrinterface\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevalr_expr_with_visible\u001B[49m\u001B[43m(\u001B[49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m# type: ignore\u001B[39;49;00m\n\u001B[1;32m    510\u001B[0m \u001B[43m        \u001B[49m\u001B[43mr_expr\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    512\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m visible[\u001B[38;5;241m0\u001B[39m]:  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    513\u001B[0m         res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/PhD/SI_Codes/SI-CART/lib/python3.9/site-packages/rpy2/rinterface.py:192\u001B[0m, in \u001B[0;36mevalr_expr_with_visible\u001B[0;34m(expr, envir)\u001B[0m\n\u001B[1;32m    185\u001B[0m r_res \u001B[38;5;241m=\u001B[39m rmemory\u001B[38;5;241m.\u001B[39mprotect(\n\u001B[1;32m    186\u001B[0m         openrlib\u001B[38;5;241m.\u001B[39mrlib\u001B[38;5;241m.\u001B[39mR_tryEval(\n\u001B[1;32m    187\u001B[0m             r_call,\n\u001B[1;32m    188\u001B[0m             envir\u001B[38;5;241m.\u001B[39m__sexp__\u001B[38;5;241m.\u001B[39m_cdata,  \u001B[38;5;66;03m# call context.\u001B[39;00m\n\u001B[1;32m    189\u001B[0m             error_occured)\n\u001B[1;32m    190\u001B[0m )\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error_occured[\u001B[38;5;241m0\u001B[39m]:\n\u001B[0;32m--> 192\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m embedded\u001B[38;5;241m.\u001B[39mRRuntimeError(_rinterface\u001B[38;5;241m.\u001B[39m_geterrmessage())\n\u001B[1;32m    193\u001B[0m res \u001B[38;5;241m=\u001B[39m conversion\u001B[38;5;241m.\u001B[39m_cdata_to_rinterface(r_res)\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(res, ListSexpVector)\n",
      "\u001B[0;31mRRuntimeError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "coverage_dict_fig1, length_dict_fig1, MSE_dict_fig1\\\n",
    "    = terminal_inference_sim(n=200, p=5, a=1, b=2,\n",
    "                             sd_y=2,\n",
    "                             noise_sd_list=[1, 2.5, 5, 10],\n",
    "                             UV_gamma_list=[],\n",
    "                             use_nonrand=True,\n",
    "                             start=0, end=3,\n",
    "                             level=0.1, path=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:39:38.652634Z",
     "start_time": "2025-02-28T05:39:36.089953Z"
    }
   },
   "id": "8777d8f757e29461"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Columns: \n",
    "# RRT_c: RRT with external randomization N(0, (c*sd_y)^2)\n",
    "# Tree val: Tree-values\n",
    "# Naive: naive inference\n",
    "# Rows: Each row correspond to one round of simulation\n",
    "pd.DataFrame(coverage_dict_fig1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d01c5abe5810828"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "       RRT_1    RRT_2.5     RRT_5    RRT_10   Tree val     Naive\n0  10.355613   4.244548  2.537076  1.704440  25.308622  1.258967\n1  11.256963   4.538982  2.740342  2.473626   2.290091  1.166525\n2  10.773288  14.469976  2.653465  1.842266   1.871536  1.178633",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RRT_1</th>\n      <th>RRT_2.5</th>\n      <th>RRT_5</th>\n      <th>RRT_10</th>\n      <th>Tree val</th>\n      <th>Naive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10.355613</td>\n      <td>4.244548</td>\n      <td>2.537076</td>\n      <td>1.704440</td>\n      <td>25.308622</td>\n      <td>1.258967</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11.256963</td>\n      <td>4.538982</td>\n      <td>2.740342</td>\n      <td>2.473626</td>\n      <td>2.290091</td>\n      <td>1.166525</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10.773288</td>\n      <td>14.469976</td>\n      <td>2.653465</td>\n      <td>1.842266</td>\n      <td>1.871536</td>\n      <td>1.178633</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns: \n",
    "# RRT_c: RRT with external randomization N(0, (c*sd_y)^2)\n",
    "# Tree val: Tree-values\n",
    "# Naive: naive inference\n",
    "# Rows: Each row correspond to one round of simulation\n",
    "pd.DataFrame(length_dict_fig1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T03:44:07.188907Z",
     "start_time": "2025-02-28T03:44:07.181622Z"
    }
   },
   "id": "beece6f59845a437"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "      RRT_1   RRT_2.5     RRT_5    RRT_10  Tree val     Naive\n0  4.754626  5.385438  6.062667  5.715567  4.774639  4.774639\n1  5.007723  4.517161  5.284633  5.598432  4.747049  4.747049\n2  5.004523  4.477550  4.936360  5.975434  4.853868  4.853868",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RRT_1</th>\n      <th>RRT_2.5</th>\n      <th>RRT_5</th>\n      <th>RRT_10</th>\n      <th>Tree val</th>\n      <th>Naive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.754626</td>\n      <td>5.385438</td>\n      <td>6.062667</td>\n      <td>5.715567</td>\n      <td>4.774639</td>\n      <td>4.774639</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.007723</td>\n      <td>4.517161</td>\n      <td>5.284633</td>\n      <td>5.598432</td>\n      <td>4.747049</td>\n      <td>4.747049</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.004523</td>\n      <td>4.477550</td>\n      <td>4.936360</td>\n      <td>5.975434</td>\n      <td>4.853868</td>\n      <td>4.853868</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns: \n",
    "# RRT_c: RRT with external randomization N(0, (c*sd_y)^2)\n",
    "# Tree val: Tree-values\n",
    "# Naive: naive inference\n",
    "# Rows: Each row correspond to one round of simulation\n",
    "pd.DataFrame(MSE_dict_fig1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T03:44:12.917381Z",
     "start_time": "2025-02-28T03:44:12.903676Z"
    }
   },
   "id": "6b98704bde23e735"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Replicating Figure 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "904e5e2222d6d69e"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th simulation\n",
      "1 th simulation\n",
      "2 th simulation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yilingh/Desktop/PhD/SI_Codes/SI-CART/Utils/discrete_family.py:148: RuntimeWarning: overflow encountered in exp\n",
      "  self._partition *= np.exp(_largest)\n"
     ]
    }
   ],
   "source": [
    "coverage_dict_fig2, length_dict_fig2, MSE_dict_fig2\\\n",
    "    = terminal_inference_sim(n=200, p=5, a=1, b=2,\n",
    "                             sd_y=2,\n",
    "                             noise_sd_list=[1],\n",
    "                             UV_gamma_list=[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                             use_nonrand=False,\n",
    "                             start=0, end=3,\n",
    "                             level=0.1, path=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:44:56.835857Z",
     "start_time": "2025-02-28T05:39:43.501598Z"
    }
   },
   "id": "9ef1b54140dc129b"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "      RRT_1    UV_0.1    UV_0.2    UV_0.3    UV_0.4    UV_0.5\n0  0.857143  1.000000  0.833333  0.857143  1.000000  0.833333\n1  0.666667  0.833333  1.000000  1.000000  0.833333  1.000000\n2  0.333333  1.000000  1.000000  0.833333  1.000000  1.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RRT_1</th>\n      <th>UV_0.1</th>\n      <th>UV_0.2</th>\n      <th>UV_0.3</th>\n      <th>UV_0.4</th>\n      <th>UV_0.5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.857143</td>\n      <td>1.000000</td>\n      <td>0.833333</td>\n      <td>0.857143</td>\n      <td>1.000000</td>\n      <td>0.833333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.666667</td>\n      <td>0.833333</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.833333</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.333333</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.833333</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns: \n",
    "# RRT_c: RRT with external randomization N(0, (c*sd_y)^2)\n",
    "# UV_k: UV decomposition with gamma = k\n",
    "# Rows: Each row correspond to one round of simulation\n",
    "pd.DataFrame(coverage_dict_fig2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:45:25.134914Z",
     "start_time": "2025-02-28T05:45:25.098389Z"
    }
   },
   "id": "250ca3ab3bdca0a6"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "      RRT_1    UV_0.1    UV_0.2    UV_0.3    UV_0.4    UV_0.5\n0  3.901546  3.895045  2.847981  2.610729  2.221367  2.004146\n1  4.204104  3.881247  2.857391  2.431454  2.153402  2.011266\n2  4.843287  3.892569  2.878134  2.449271  2.205467  2.020659",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RRT_1</th>\n      <th>UV_0.1</th>\n      <th>UV_0.2</th>\n      <th>UV_0.3</th>\n      <th>UV_0.4</th>\n      <th>UV_0.5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.901546</td>\n      <td>3.895045</td>\n      <td>2.847981</td>\n      <td>2.610729</td>\n      <td>2.221367</td>\n      <td>2.004146</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.204104</td>\n      <td>3.881247</td>\n      <td>2.857391</td>\n      <td>2.431454</td>\n      <td>2.153402</td>\n      <td>2.011266</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.843287</td>\n      <td>3.892569</td>\n      <td>2.878134</td>\n      <td>2.449271</td>\n      <td>2.205467</td>\n      <td>2.020659</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns: \n",
    "# RRT_c: RRT with external randomization N(0, (c*sd_y)^2)\n",
    "# UV_k: UV decomposition with gamma = k\n",
    "# Rows: Each row correspond to one round of simulation\n",
    "pd.DataFrame(length_dict_fig2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:45:26.923646Z",
     "start_time": "2025-02-28T05:45:26.917141Z"
    }
   },
   "id": "ed46cebfb74768c5"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "      RRT_1    UV_0.1    UV_0.2    UV_0.3    UV_0.4    UV_0.5\n0  4.754626  5.724732  5.861563  4.959129  5.453345  5.903943\n1  5.007723  4.709248  4.827129  4.688048  4.627214  4.768680\n2  5.004523  4.553102  4.918423  4.947749  5.058473  5.104218",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RRT_1</th>\n      <th>UV_0.1</th>\n      <th>UV_0.2</th>\n      <th>UV_0.3</th>\n      <th>UV_0.4</th>\n      <th>UV_0.5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.754626</td>\n      <td>5.724732</td>\n      <td>5.861563</td>\n      <td>4.959129</td>\n      <td>5.453345</td>\n      <td>5.903943</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.007723</td>\n      <td>4.709248</td>\n      <td>4.827129</td>\n      <td>4.688048</td>\n      <td>4.627214</td>\n      <td>4.768680</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.004523</td>\n      <td>4.553102</td>\n      <td>4.918423</td>\n      <td>4.947749</td>\n      <td>5.058473</td>\n      <td>5.104218</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns: \n",
    "# RRT_c: RRT with external randomization N(0, (c*sd_y)^2)\n",
    "# UV_k: UV decomposition with gamma = k\n",
    "# Rows: Each row correspond to one round of simulation\n",
    "pd.DataFrame(MSE_dict_fig2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:39:10.887445Z",
     "start_time": "2025-02-28T05:39:10.884503Z"
    }
   },
   "id": "12e6df0dc95ead2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "20c87f9deb7dfc3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
